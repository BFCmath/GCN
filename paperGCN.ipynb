{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paper link: [here](https://arxiv.org/pdf/1609.02907)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A summary for what I can understand from the paper. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization \n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*Y3IcRT75O6f2NC9BFGhq3g.png\" width=\"600\" height=\"300\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "![demo](image/demo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Meaning of the convolutions on Graph "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "![convolution function](image/convolution_function.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "+ A: adjacency matrix \n",
    "+ $\\bar{A}$: adjacency matrix with self-connections\n",
    "+ D: $D_{ii} = \\sum{A_i}$ \n",
    "+ H = X\n",
    "+ W: weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ex1](image/ex1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ex2](image/ex2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### What is D "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "the D is actually a way to normalize the A (turning them into prob matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**Why not $D^{-1}*A$ but $D^{-0.5}AD^{-0.5}$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "both way can normalize A, but the paper prefers the later because it can reduce gradient exploding/vanishing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### The similarity between GCN and CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/GCN_vs_CNN_overview.png\" width=\"500\" height=\"300\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "the $D^{-0.5}AD^{-0.5}$ is just masked as 1 if two nodes are neighbors, and then normalize it to calculate the proba.  \n",
    "Another way to understand this is think its like the following kernel:\n",
    "\n",
    "$ \\frac{1}{9} \\frac{1}{9} \\frac{1}{9}$  \n",
    "\n",
    "$ \\frac{1}{9} \\frac{1}{9} \\frac{1}{9}$  \n",
    "\n",
    "$ \\frac{1}{9} \\frac{1}{9} \\frac{1}{9}$  \n",
    "\n",
    "\n",
    "its calculate the avg between the current cell to surrounding, and the convolution calculation in the graph is exactly the same!!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Treating the GNN just like CNN with the number of node the same in each layer and the vector inside of the node will gradually reduce, we can extract the last layer as a feature-space-extracted matrix (just like CNN!!!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "![model](https://miro.medium.com/v2/resize:fit:1400/1*7vMs4Ym_W1FlqcuI7PulSQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SemiSupervised Graph problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![problem](image/problem.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drawback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "+ Memory requirement: dont support mini-batch\n",
    "+ Directed edges and edge features: just undirected graph\n",
    "+ Limiting assumption: all contribute is the same\n",
    "+ Transductive setting: retrain when add nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphSage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "an inductive learning method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "paper [here](https://arxiv.org/abs/1706.02216)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](image/graphsage.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "##  Inductive and tranductive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ inductive: adaptable to unseen data\n",
    "+ tranductive: unadaptable to unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](image/embed_GSAGE.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "+ k: for each aggregate (could be mean, pool or lstm,...)\n",
    "+ v: nodes in graph\n",
    "+ for each v, do aggregate(all previous value of neighbor nodes)\n",
    "+ then concat the previous value of v with that result, then go through non-linear activate function \n",
    "+ normalize of v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![gat](image/GAT.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation: \n",
    "+ so in GCN and GraphSage, we notice that the $a_vu$ is 1/len, which means that all the weight of neighbor nodes is basically the same\n",
    "+ In GAT, the $a_vu$ will be defined based on the importance of the neighbor nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention in GAT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define a as attention-compute function, we can have e like this:\n",
    "\n",
    "![attention1](image/attention1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the e using softmax, we get $a_vu$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "a can be define as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![attention2](image/attention2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information about attentions, revise the old note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![attention3](image/attention3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relevant to TRANSFORMER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "revise the old note for transformer.\n",
    "\n",
    "We can clearly see that transformer is just like a GNN with fully connected node (without positional layer and more complex attention mechanism)\n",
    "\n",
    "Also, because of that, we can apply multi-head attention and so more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caution\n",
    "1. Be cautious when stacking GCN layers \n",
    "Unlike others, GCN will lead to Oversmoothing (more hops can lead to all the vector are the same) \n",
    "\n",
    "+ Sol 1: add more aggregation\n",
    "+ Sol 2: injecting MLP between GCN\n",
    "2. Add skip connection (just like in Transformer)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
